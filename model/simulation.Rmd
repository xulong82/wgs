---
title: "How priors on $p/se(p)$ affect a variant's posterior effect?"
author: "Xulong Wang"
date: "January 18, 2016"
output: pdf_document
---

```{r, include = F}

library(rstan)
library(dplyr)

```

## Background

Model: Logistic regression: $logit(p) \sim p * genotype$  
Response: case and control (0/1)  
Predictor: numerical genotypes in 0/1/2  

## Model and data 

```{r}

rm(list = ls())
setwd("~/Dropbox/GitHub/wgs/model")

N = 1e2

set.seed(3)
g = sample(0:2, N, replace = T)
y = sample(0:1, N, replace = T)
cor(g, y)

(mod = stan_model("./simulation.stan"))
(dat = list(N = N, y = y, g = g, prior = 0))

```

## Fitting model with a flat prior: $p/se(p) = 0$

```{r}

fit = optimizing(mod, hessian = T, algorithm = "LBFGS", data = dat)
se = sqrt(diag(solve(-fit$hessian)))["p"]
(x = c(fit$par, se = se))

```

## Fitting model with testing priors

```{r, message = F, warning = F}

t1 = seq(-15, 15, 0.1)

```

```{r, include = F}

test = sapply(t1, function(t1) {
  cat(t1)
  dat = within(dat, { prior = t1 })
  fit = optimizing(mod, hessian = T, algorithm = "LBFGS", data = dat)
  se = sqrt(diag(solve(-fit$hessian)))["p"]
  c(fit$par, se = se)
})

test$pvalue = pnorm(abs(test$p), sd = test$se.p, lower.tail = F) * 2

(test = as.data.frame(t(test))) %>% head

```

## Ouputs

Absolutes of $p/se(p)$ were used for significance quantity. 

Abbreviations of statistics:

1. $t0$ is the estiamted $p/se(p)$ with a flat prior  
2. $t1$ is the prior $p/se(p)$
3. $t2$ is the estimated $p/se(p)$ with $t1$ priors

```{r}

t0 = x[2] / x[6]
t2 = test$p / test$se.p
p1 = pnorm(abs(x["p"]), sd = x["se.p"], lower.tail = F) * 2

```

## Estimates with flat and $t1$ priors: t0 vs t2

```{r}

pdf(file = "simulation.pdf", height = 4, width = 6)

plot(t1, test$p, xlab = "Prior Standardized Effect Size", ylab = "Post Effect Size")
abline(h = x["p"])

plot(t1, test$se.p, xlab = "Prior Standardized Effect Size", ylab = "Post Standard Error of Effect Size")
abline(h = x["se.p"])

plot(t1, t2, xlab = "Prior Standardized Effect Size", ylab = "Post Standardized Effect Size")
abline(h = t0)

plot(t1, test$pvalue, xlab = "Prior Standardized Effect Size", ylab = "Post P-value",
     col = as.numeric(test$pvalue < p1) + 2)
legend("topleft", c("Decreased Post P-value", "Increased Post P-value"), fill = c("green", "red"))
abline(h = p1)

dev.off()

```

1. Posterior p-values improved when prior $t1$ was at the same direction of $t0$  
2. Posterior p-values deteoriated when prior $t1$ was contradict with $t0$   
3. Extreme prior $t1$ can dominate the data 

## Prior and post: t1 vs t2

Red: abs(t2) > abs(t1)
Blk: abs(t2) < abs(t2)

```{r}

plot(t1, t2, col = as.numeric(abs(t2) < abs(t1)) + 2)
abline(v = max(t1[abs(t2) > abs(t1)]), col = "red")
abline(v = min(t1[abs(t2) > abs(t1)]), col = "red")
legend("topleft", c("abs(t2) > abs(t1)", "abs(t2) < abs(t1)"), fill = c("red", "black"))
abline(h = t0)

# t1[abs(t2) > abs(t1)]

```

$abs(t2) > abs(t1)$ happened with weaker priors, where data dominated the estimations. 

Took the last two graphs together, we saw situations where $abs(t2)$ was smaller than both $abs(t0)$ and $abs(t1)$, and it happend when the $t1$ was modestly contradict with $t0$.

Graph below gave the estimated $se(p)$ versus prior $t1$. Red was abs(t2) > abs(t0). Blk was abs(t2) < abs(t0). Standard error was high when evidences from prior and data were comparable and contradicting.

```{r}

plot(t1, test$se.p, pch = 3, col = as.numeric(abs(t2) > abs(t0)) + 1)
legend("topleft", c("abs(t2) > abs(t0)", "abs(t2) < abs(t0)"), fill = c("red", "black"))
abline(h = x["se.p"])

```

# The t estimate leaned too much onto the prior

```{r, include = F}

plot(t1, test$t, pch = 3, col = as.numeric(abs(test$t) > t0) + 1)
t1[ abs(test$t) > t0 ]

plot(test$p)
plot(test$se.p)
abline(h = x[1])

plot(t1, test$t)
abline(0, 1)

```